<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Differentiable Simulator | Qisi Wang</title><link>https://cheesew.github.io/tags/differentiable-simulator/</link><atom:link href="https://cheesew.github.io/tags/differentiable-simulator/index.xml" rel="self" type="application/rss+xml"/><description>Differentiable Simulator</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Mon, 19 Jul 2021 00:00:00 +0000</lastBuildDate><image><url>https://cheesew.github.io/media/icon_hu_645fa481986063ef.png</url><title>Differentiable Simulator</title><link>https://cheesew.github.io/tags/differentiable-simulator/</link></image><item><title>Learning active quasistatic physics-based models from data</title><link>https://cheesew.github.io/publication/sig/</link><pubDate>Mon, 19 Jul 2021 00:00:00 +0000</pubDate><guid>https://cheesew.github.io/publication/sig/</guid><description>&lt;p>Humans and animals can control their bodies to generate a wide range of motions via low-dimensional action signals representing high-level goals. As such, human bodies and faces are prime examples of active objects, which can affect their shape via an internal actuation mechanism. This paper explores the following proposition: given a training set of example poses of an active deformable object, can we learn a low-dimensional control space that could reproduce the training set and generalize to new poses?&lt;/p>
&lt;p>In contrast to popular machine learning methods for dimensionality reduction such as auto-encoders, we model our active objects in a physics-based way. We utilize a differentiable, quasistatic, physics-based simulation layer and combine it with a decoder-type neural network. Our differentiable physics layer naturally fits into deep learning frameworks and allows the decoder network to learn actuations that reach the desired poses after physics-based simulation. In contrast to modeling approaches where users build anatomical models from first principles, medical literature or medical imaging, we do not presume knowledge of the underlying musculature, but learn the structure and control of the actuation mechanism directly from the input data.&lt;/p>
&lt;p>We present a training paradigm and several scalability-oriented enhancements that allow us to train effectively while accommodating high-resolution volumetric models, with as many as a quarter million simulation elements. The prime demonstration of the efficacy of our example-driven modeling framework targets facial animation, where we train on a collection of input expressions while generalizing to unseen poses, drive detailed facial animation from sparse motion capture input, and facilitate expression sculpting via direct manipulation.&lt;/p>
&lt;video controls width="100%" poster="/video/SIG1.png">
&lt;source src="https://cheesew.github.io/video/SIG1.mp4" type="video/mp4">
Your browser does not support the video tag.
&lt;/video>
&lt;video controls width="100%" poster="/video/SIG2.png">
&lt;source src="https://cheesew.github.io/video/SIG2.m4v" type="video/mp4">
Your browser does not support the video tag.
&lt;/video></description></item></channel></rss>